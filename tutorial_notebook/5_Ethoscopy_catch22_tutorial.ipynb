{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethoscopy - Behavpy to catch22"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Catch22 is a shortened version of HCTSA, a time series comparative analysis toolbox in matlab. Catch22 has been adapted to also work in python and uses the top 22 most used analytical tests from the 100s used in HCTSA. \n",
    "\n",
    "#### See heere for more details: https://feature-based-time-series-analys.gitbook.io/catch22-features/\n",
    "#### or their github: https://github.com/DynamicsAndNeuralSystems/pycatch22"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ethoscopy as etho\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pycatch22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the data from the circadian tutorial\n",
    "from ethoscopy.misc.get_tutorials import get_tutorial\n",
    "data, metadata = get_tutorial('circadian')\n",
    "df = etho.behavpy(data, metadata, check = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Some data curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll look at movement here\n",
    "var = 'moving'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most basic curation is to pick a specific time period\n",
    "df = df.t_filter(start_time = 0, end_time = 9*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use interpolate to fill in the missing data points, this can be useful if it's only a few points missing per specimen\n",
    "df = df.interpolate(variable = var, step_size = 60, t_column = 't')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or we can also group several rows together by increasing the t diff, here we increase\n",
    "# from 60 to 120, so we find the average of every two rows\n",
    "\n",
    "## We won't run this here, but keep it in mind for the future if you have too few data points \n",
    "# df = df.bin_time(column = var, bin_secs =  120, function = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call .summary() to check the amount of data points\n",
    "df.summary(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we've completed our curation or if we just want to remove specimens that don't have enough values,\n",
    "# we can call curate to remove all specimens with too few points still\n",
    "# We can see from summary() that the normal amount is 11999\n",
    "df = df.curate(points = 11999)\n",
    "\n",
    "# Note: The interpolate method returns rows 1 shorter than before so you'll need to add a minus 1 if using curate after\n",
    "# Note: If you've called the above bin_time this curate will return an empty dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using x position data the interesting part is how the fly positions itself in relation to the food\n",
    "# However this will be different on the x,y axis for flies on either side of the ethoscope, so lets normalise it\n",
    "# You only need to run this is using the x variable\n",
    "df_r = df.xmv('region_id', list(range(11,21)))\n",
    "df_l = df.xmv('region_id', list(range(1,11)))\n",
    "df_r['x'] = 1 - df_r['x']\n",
    "df = df_l.concat(df_r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalise the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ethoscope data can sometimes do with some augmentaion to make sure it perfroms better in Catch22\n",
    "#### Try running your data unnormalised and normalised, and with different methods to see the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catch22 takes only lists, but to normalise the data we'll have to put it into a numpy array first\n",
    "list_x = df.groupby(df.index, sort = False)[var].apply(list)\n",
    "arr_x = np.array([np.array(x) for x in list_x])\n",
    "# Here we grab the ids of each for the labels that we'll use later\n",
    "list_id = list_x.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use some or all of these functions to normalise the data between specimens\n",
    "\n",
    "# norm01 transforms the data to be between 0 and 1\n",
    "def norm01(x):\n",
    "    return (x-np.nanmin(x))/(np.nanmax(x)-np.nanmin(x))\n",
    "# or\n",
    "# find the zscore\n",
    "def zscore(x):\n",
    "    return (x-np.mean(x))/(np.std(x))\n",
    "\n",
    "# Only use this if looking at phi, it changes it be only from 0-90 or horizontal to veritcal as the ethoscope doesn't track direction\n",
    "def norm_phi(x):\n",
    "    return np.where(x > 90, 90 - (x - 90), x)\n",
    "\n",
    "# Smooth out the time series data\n",
    "def moving_average(a, n) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As the data here is just 1s and 0s (True and False) we don't want to perform any curation. Head to tutorial 6 for information on how to apply it to other sets of data. But for now just bear in mind that it's a useful pre-processing tool."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Catch22 and store in a behavpy dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the time series data is augmented to fit into the correct format, a nested list, and run through the catch22 function: pycatch22.catch22_all()\n",
    "# You can also call individual tests if you know which ones you need, see the pycatch documents for more details\n",
    "# If you want the mean and std then call catch24 = True within the function \n",
    "data = [pycatch22.catch22_all(list_x[i])['values'] for i in range(len(list_x))]\n",
    "cols = pycatch22.catch22_all(list_x[0])['names']\n",
    "\n",
    "c22 = pd.DataFrame(data, columns = cols, index = list_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To compare the scores and find trends we need to normalize and standardise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn is a common python package for statistics and machine learning, if you dont have in installed, install from pip now.\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We call the normalisation across each variable (column)\n",
    "# normalize() scales vectors to a unit norm so that the vector has a length of 1\n",
    "normed = normalize(c22, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We scale the data so all scores are fitted to between 0 and 1 (classification techniques perforn much better with same scaled data)\n",
    "scaled = MinMaxScaler().fit_transform(normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets put the data into a dataframe and then a behavpy dataframe\n",
    "c22_norm = pd.DataFrame(scaled, columns = c22.columns, index = list_id)\n",
    "c22_norm.index.name = 'id'\n",
    "c22_df = etho.behavpy(c22_norm, df.meta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Viewing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use matplotlib and its add on pacakage seaborn here to create graphs rather than the usual plotly\n",
    "from matplotlib import pyplot as plt\n",
    "# You may need to pip install seaborn\n",
    "import seaborn as sns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmaps are often the best way to get a quick view of where your outputs vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = sns.heatmap(c22_df, cmap='PiYG')\n",
    "heatmap.set_title('Heatmap of normed output', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see a the two histogram mode tests all score roughly the same, so they're not likely to be useful. However, both the CO seem to have a variation in scores across the board. Lets look at the heatmap for each group we have in the metadata for circadian length: short, WT, and long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We iterate and filter with xmv to get each group, removing the y ticks so its less cluttered \n",
    "f, ax = plt.subplots(3,1, sharex='col')\n",
    "f.subplots_adjust(hspace=0.3)\n",
    "\n",
    "for c, typ in enumerate(['short', 'wt', 'long']):\n",
    "    tmp_df = c22_df.xmv('period_group', typ)\n",
    "    g = sns.heatmap(tmp_df, cmap=\"PiYG\", cbar=False, ax=ax[c])\n",
    "    g.set(yticklabels=[])  \n",
    "    g.set(title=typ)\n",
    "    g.set(ylabel=None)\n",
    "    g.tick_params(left=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see clearly the difference between the groups with the CO analysis. As well as big differences between long to WT and short when looking at FC local Simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also plot the correaltion bewteen each variable to see the relationship between them all\n",
    "\n",
    "heatmap = sns.heatmap(c22_df.corr(), vmin=-1, vmax=1, cmap = sns.diverging_palette(220, 10, as_cmap=True))\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are strong clusters of tests that are similiar to each other which is to be expected. But we can also see CO ters are strong correlated both postively and negatively with many of the others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need the labels to colour code the plot, but to make sure they have the same order we'll match them to the main df and remove them\n",
    "# The order in the metadata is often (and should be) the same as order in the data, but sometimes with curation it can change so this is just to be safe\n",
    "# We make a dictionary of the id and the label and map it to data\n",
    "label_dict = c22_df.meta['period_group'].to_dict()\n",
    "c22_df['label'] = c22_df.index.to_series().map(label_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A pairwise plot is a more visual version of th correlation plot, but we can also colour by our labels and see how they group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise plot will give you the most insight into which variables have distinct populations per label\n",
    "sns.pairplot(c22_df, vars = cols, hue = 'label')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see just visually from the pairwise plot what most of the variable combinations serperate the populations into roughly distint groups, which bodes well for classification. We can evene see a few with linear correlations such as IN_AutoMutualInfoStats_40_gaussian_fmmi X CO_FirstMin_ac."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the pairwise plot we can see that the tests can likely seperate the groups well. We can visualise this all together with a PCA or TSNE plots. Both theses plots take all the variables are reduce them to a lower dimmentiality at the cost of some detail, often 2 or 3 as we can then visulise them. In the reduction the algorithm will creat new components that represent the most valuable variables.\n",
    "\n",
    "#### There are a few differences between PCA and TSNE, but in short TSNE works better with non-linear relationships and can group nearby points better. However, it does this at the cost of true distance between groups and over represent differences between clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import both functions from sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets turn each row into a list and nest them, we also need to drop the labels as they are not numerical\n",
    "X = c22_df.drop(columns = ['label']).values.tolist()\n",
    "X = np.array([np.array(i) for i in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll intialise the PCA function and state we want it reduced to 2 variables\n",
    "pca = PCA(n_components=2)\n",
    "# fit_transform() learns the data and then transforms it to the 2 principle components.\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the output into a pandas datafram and add the labels\n",
    "pca_df = pd.DataFrame(X_pca, columns = ['PCA1', 'PCA2'])\n",
    "pca_df['label'] = c22_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "sns.scatterplot(data=pca_df, \n",
    "                x=\"PCA1\", \n",
    "                y=\"PCA2\", \n",
    "                hue=\"label\")\n",
    "\n",
    "plt.title(\"PCA Plot\",\n",
    "        fontsize=14)\n",
    "plt.xlabel('First Principal Component',\n",
    "        fontsize=10)\n",
    "plt.ylabel('Second Principal Component',\n",
    "        fontsize=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see the PCA analysis has 3 distinct groups with some of the WT labelled flies in the short cluster. All long labelled flies are seperate and easy to distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat what we did above but with the TSNE function\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_ts = tsne.fit_transform(X)\n",
    "ts_df = pd.DataFrame(X_ts, columns = ['TS1', 'TS2'])\n",
    "ts_df['label'] = c22_df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    " \n",
    "sns.scatterplot(data=ts_df, \n",
    "                x=\"TS1\", \n",
    "                y=\"TS2\", \n",
    "                hue=\"label\")\n",
    " \n",
    "plt.title(\"TSNE Plot\",\n",
    "        fontsize=14)\n",
    "plt.xlabel('First TSNE component',\n",
    "        fontsize=10)\n",
    "plt.ylabel('Second TSNE component',\n",
    "        fontsize=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A similar output to the PCA with long being clustered seperately and some mixing of the short and WT labels in two clusters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the above plots it looks like the dataset is good for some classification attempts, we'll go through two of the powerful but simple techniques SVM and Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to load in a few more bits from sklearn\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets put our labels into variable y\n",
    "y = c22_df['label'].tolist()\n",
    "# And split our dataset into both a training set (70%) and testing set (30%)\n",
    "# Stratify makes sure there is the same ratio of all groups in both train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function prints out accuracy and reports of the classifier\n",
    "def print_score(clf, X, y):\n",
    "        pred = clf.predict(X)\n",
    "        clf_report = pd.DataFrame(classification_report(y, pred, output_dict=True))\n",
    "        print(f\"Accuracy Score: {accuracy_score(y, pred) * 100:.2f}%\")\n",
    "        print(\"_______________________________________________\")\n",
    "        print(\"Classification Report:\")\n",
    "        print(clf_report)\n",
    "        print(\"_______________________________________________\")\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(confusion_matrix(y, pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines (SVM) is a machine learning algorithm that creates a decision boundry in the variable space that it thinks best seperates your given groups. We'll kick off by just calling the most basic SVM, one that employs a linear kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier and fit it to the data\n",
    "svm_clf = SVC(kernel='linear')\n",
    "svm_clf.fit(X_train, y_train)\n",
    "# The train dataset score\n",
    "print_score(svm_clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The test dataset score\n",
    "print_score(svm_clf, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The linear SVM performs very well on the training dataset (which is to be assumed), but has a poor score for the test dataset. We could play around with the settings for SVM function manually or we can perform a grid search. A grid search takes a list of different inputs to the classifying function and runs all the combination, finding the one with the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary of the parameters you want to change and the list of variabels to apply\n",
    "# We'll use all the kernels avaiable to us and a range of gamma and C\n",
    "# Head to the sklearn wiki for information on tthe parameters\n",
    "param_grid = {'C': [0.01, 0.1, 0.5, 1, 5, 10, 100], \n",
    "            'gamma': [1, 0.75, 0.5, 0.25, 0.1, 0.01, 0.001, 'auto'], \n",
    "            'kernel': ['rbf', 'poly', 'linear']} \n",
    "# Now we setup the Grid search with the SVM, the parameters. CV is a resampling method that test and trains within the given dataset\n",
    "grid_svm = GridSearchCV(SVC(), param_grid, verbose=True, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the data to the gridsearch\n",
    "grid_svm.fit(X_train, y_train)\n",
    "params_svm = grid_svm.best_params_\n",
    "print(f\"Best params: {params_svm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take the parameters and score then\n",
    "svm_clf = SVC(**params_svm)\n",
    "svm_clf.fit(X_train, y_train)\n",
    "print('Training score:')\n",
    "print_score(svm_clf, X_train, y_train)\n",
    "print('Testing score:')\n",
    "print_score(svm_clf, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The grid searched parameters perform worse on the trained dataset, but better on the test set. Meaning the it's not as overfitted to the training set!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets have a look at an SVM when applied to the PCA results from above to gain a visual understanding of what's going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to encode the labels as numbers to colour code them with the plotting function below\n",
    "le = LabelEncoder()\n",
    "le.fit(['short', 'wt', 'long'])\n",
    "y2 = le.transform(pca_df.label.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll take the values for the PCA dataframe, and the set the labels a yp\n",
    "Xp = pca_df.values[:, :2] \n",
    "yp = y2\n",
    "\n",
    "# Creates a grid for the plot points to applied to\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    x_min, x_max = x.min() - 0.1, x.max() +0.1\n",
    "    y_min, y_max = y.min() - 0.1, y.max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "# This function plots the different areas the SVM decides for each label in the variable space\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "# Call a grid search on this PCA values and call the best fit model\n",
    "grid.fit(Xp, yp)\n",
    "best_params = grid.best_params_\n",
    "model = SVC(**best_params)\n",
    "clf = model.fit(Xp, yp)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "title = ('Decision surface of the best fit SVM')\n",
    "# Set-up grid for plotting.\n",
    "X0, X1 = Xp[:, 0], Xp[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "# Plot the svm areas and variable points\n",
    "plot_contours(ax, clf, xx, yy, cmap = 'coolwarm', alpha=0.8)\n",
    "ax.scatter(X0, X1, c=yp, s=20, cmap = 'coolwarm', edgecolors='k')\n",
    "ax.set_ylabel('PCA 2')\n",
    "ax.set_xlabel('PCA 1')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_title(title)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This visuliases nicely the problem the SVM is having with classifing at 100%, it's the points we noticed above that are WT but have short sleeping like phenotype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we'll look at random forest classifers (RFC). RFC's are a collection of tree classifiers, tree classifiers work by \"bagging\" whereby in training decisions are made to bag/collect certain labels due to an internal decision. On their own they can oftern overfit to the training dataset, however as a forest and added randomness this can be accounted for. Random forests often perform the best for a wide range of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets intialise the classifer and create a grid search for the off\n",
    "# Head to the sklearn wiki to understand the parameters\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "param_grid = {'n_estimators': [100, 200, 300, 500],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'max_depth' : [4,5,6,7,8,10],\n",
    "            'criterion' :['gini', 'entropy']}\n",
    "grid_rfc = GridSearchCV(rfc, param_grid, refit=True, verbose=1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call and fit the grid search for the RFC\n",
    "grid_rfc.fit(X_train, y_train)\n",
    "params_rfc = grid_rfc.best_params_\n",
    "print(f\"Best params (RFC): {params_rfc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see how it's done\n",
    "rfc_clf = RandomForestClassifier(**params_rfc, random_state = 42)\n",
    "rfc_clf.fit(X_train, y_train)\n",
    "print('Scores for the trained dataset:')\n",
    "print_score(rfc_clf, X_train, y_train)\n",
    "print('Scores for the test dataset:')\n",
    "print_score(rfc_clf, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sadly it's not performed as well as the best SVM. We could play around with more parameters, but for now it looks like the SVM score of 80% is the best we'll get (which is still pretty good)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you'd like to perform more tests look online for tutorials about the other classifiers in the sklearn ecosystem. You can also head to the HCTSA tutorial page and look to copy some of their techniques from matlab to python: https://hctsa-users.gitbook.io/hctsa-manual/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64774a8dfb1bd896c6efea99b1b4772a6458a05741a63e1cad6fc82c0bcee224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
